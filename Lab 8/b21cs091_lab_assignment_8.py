# -*- coding: utf-8 -*-
"""B21CS091_Lab_Assignment_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cA_KgS63Pcwfw36TnvRWyumi5CCQoFWD

#**Question 1**
"""

import numpy as np
import pandas as pd

!pip install mlxtend
!pip install mlxtend --upgrade --no-deps
from mlxtend.feature_selection import SequentialFeatureSelector

"""Part 1"""

data = pd.read_csv('/content/train.csv')

data.shape

data.head()

data.info()

import matplotlib.pyplot as plt

fig, ax = plt.subplots(5, 6, figsize = (20, 20))

fig.suptitle('Histograms of Train Data')
for i, col in enumerate(data.columns):
    ax[i//6, i%6].hist(data[col],  edgecolor='black')
    ax[i//6, i%6].set_title(col)

"""Remove the 'id' and 'Unnamed : 0' columns."""

data = data.drop(data.iloc[:,[0, 1]], axis = 1).copy()



categorical_indexes = [0, 1, 3, 4] + list(range(6, 20))
data.iloc[:,categorical_indexes] = data.iloc[:,categorical_indexes].astype('category')

data.info()

print("Count of NaN values\n")
data.isna().sum()

data['Arrival Delay in Minutes'].fillna(data['Arrival Delay in Minutes'].median(axis = 0), inplace = True)

print("Totol Number of NA values in complete Data : ", data.isna().sum().sum())

data.describe()

from sklearn.preprocessing import LabelEncoder

LE = LabelEncoder()


cat_columns = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']
data_trans = data.copy()

for c in cat_columns:
    data_trans[c] = LE.fit_transform(data[c])

Y = data_trans['satisfaction']
X = data_trans.drop(columns = ['satisfaction'], axis = 1).copy()

print(X.shape)
print(Y.shape)

X.head()

"""Part 2"""

from sklearn.tree import DecisionTreeClassifier

DTG = DecisionTreeClassifier()

SFS = SequentialFeatureSelector(DTG, 10,forward = True, floating = False , scoring = "accuracy")
print(SFS)

SFS.fit(X, Y)

print(f"The Accuray of all 10 featues is : {SFS.k_score_*100}%")

print("Names of Selected Features are :")
SFS.k_feature_names_

print("Complete information of results : ")
SFS.subsets_[10]

"""Part 3"""

SFS_2 = SequentialFeatureSelector(DTG, 10, forward = True, floating = False , scoring = "accuracy", cv = 4)


SFFS = SequentialFeatureSelector(DTG, 10, forward = True, floating = True , scoring = "accuracy", cv = 4)

SBFS = SequentialFeatureSelector(DTG, 10, forward = False, floating = True , scoring = "accuracy", cv = 4)

SFS_2.fit(X, Y)

DTG2 = DecisionTreeClassifier()

SBS = SequentialFeatureSelector(DTG2, 10, forward = False, floating = False , scoring = "accuracy", cv = 4)

SBS.fit(X, Y)

SFFS.fit(X, Y)

SBFS.fit(X, Y)

print('The CV scores of SFS :', SFS_2.subsets_[10]['cv_scores'])
print('The CV scores of SBS :', SBS.subsets_[10]['cv_scores'])
print('The CV scores of SFFS :', SFFS.subsets_[10]['cv_scores'])
print('The CV scores of SBFS :', SBFS.subsets_[10]['cv_scores'])

import matplotlib.pyplot as plt
 
x = list(range(1, 5))

plt.figure(figsize=(10,8))
plt.subplot(2, 2, 1)
plt.plot(x, SFS_2.subsets_[10]['cv_scores'])
plt.title("SFS")


plt.subplot(2, 2, 2)
plt.plot(x, SBS.subsets_[10]['cv_scores'])
plt.title("SBS")



plt.subplot(2, 2, 3)
plt.plot(x, SFFS.subsets_[10]['cv_scores'])
plt.title("SFFS")


plt.subplot(2, 2, 4)
plt.plot(x, SBFS.subsets_[10]['cv_scores'])
plt.title("SBFS")

dataframe_1 = pd.DataFrame.from_dict(SFS_2.get_metric_dict()).T
dataframe_1

dataframe_2 = pd.DataFrame.from_dict(SBS.get_metric_dict()).T
dataframe_2

dataframe_3 = pd.DataFrame.from_dict(SFFS.get_metric_dict()).T
dataframe_3

dataframe_4 = pd.DataFrame.from_dict(SBFS.get_metric_dict()).T
dataframe_4

from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs


plot_sfs(SFS_2.get_metric_dict(), kind='std_err')
plt.title('SFS')
plt.show()

plot_sfs(SBS.get_metric_dict(), kind='std_err')
plt.title('SBS')
plt.show()

plot_sfs(SFFS.get_metric_dict(), kind='std_err')
plt.title('SFFS')
plt.show()

plot_sfs(SBFS.get_metric_dict(), kind='std_err')
plt.title('SBFS')
plt.show()



"""###**Part 6**"""

from sklearn.model_selection import train_test_split as tts
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

#Also since we have just 2 Class.

def confusion_matrix(model,X,Y):
  Y_pred = model.predict(X)
  Y_t = np.array(Y)

  tp = (Y_pred[Y_pred == Y] == 1).sum() 
  fp = (Y_pred[Y_pred != Y] == 1).sum() 
  tn = (Y_pred[Y_pred == Y] == 0).sum() 
  fn = (Y_pred[Y_pred != Y] == 0).sum() 

  return tp,fp,fn,tn

def accuracy(model,X,Y):
    tp,fp,fn,tn = confusion_matrix(model,X,Y)
    return (tp + tn)/(tp+fp+tn+fn)

"""###**Using information gain**

"""

def entropy(x,y):
    temp_sum = x+y +0.0000001          #adding small number to neglect error of zero division
    a = x/temp_sum +0.0000001
    b = y/temp_sum +0.0000001  
    return -a*np.log2(a) - b*np.log2(b)


def info_gain(model,X,Y):
    tp,fp,fn,tn = confusion_matrix(model,X,Y)

    return entropy(tp+fn,fp+tn) - (((tp+fp)*entropy(tp,fp) + (tn+fn)*entropy(fn,tn)) / (tp+fp+tn+fn))

"""**Using euclidean_distance**

"""

def euc_dist(model,X,Y):
    Y_pred = model.predict(X)
    dist = np.sqrt(np.sum((Y_pred - Y)**2))
    return -dist

"""**Using city_block distance**

"""

def CB_distance(model,X,Y):
    Y_pred = model.predict(X)
    dist = np.sum(abs(Y_pred - Y))
    return -dist

"""**Using angular_separation**

"""

def ang_seperation(model,X,Y):
    Y_pred = model.predict(X)
    
    numer = np.sum(Y_pred*Y)
    deno1 = np.sum(np.square(Y_pred))
    deno2 = np.sum(np.square(Y)) 

    if deno1 == 0 or deno2 == 0:
      dist = np.inf
    else:
      dist = numer/np.sqrt((deno1*deno2))

    return -dist

def bi_direc_fsg(X, Y, model, check):
    
    X_train,X_test,Y_train,Y_test = tts(X,Y,train_size = 0.8)  

    best_features = set()
    remaining_features = set([i for i in X.columns])    
    best_performance = -np.inf
    curr_performance = -np.inf
    while len(remaining_features) != 0:
    
        selected_feature = None     

        for feature in remaining_features:
            S_new = best_features.union({feature})
            model.fit(X_train[list(S_new)], Y_train)      

            performance = check(model, X_test[list(S_new)], Y_test)      
            if performance > curr_performance:
                curr_performance = performance
                selected_feature = feature

        if selected_feature != None:
            best_features = best_features.union({selected_feature})
            remaining_features = remaining_features - {selected_feature}

        selected_feature = None

        for feature in best_features:
            S_new = best_features - {feature}
            if len(best_features) == 1:
                break

            model.fit(X_train[list(S_new)],Y_train)

            performance = check(model, X_test[list(S_new)], Y_test)    

            if performance > curr_performance:
                curr_performance = performance
                selected_feature = feature

        if selected_feature != None:
            best_features = best_features - {selected_feature}
            reamaining_features = remaining_features.union({selected_feature})

        if curr_performance > best_performance:
            best_performance = curr_performance
        else:
            break

    return list(best_features)



#Reducing the Size so that can get faster result in SVC

class1_indexes = np.where(Y == 1)[0].tolist()
class2_indexes = np.where(Y == 0)[0].tolist()


random_class1_indices = np.random.choice (class1_indexes, 5000, replace = False )
random_class2_indices = np.random.choice (class2_indexes, 5000, replace = False )

final_indexes = np.concatenate ([random_class1_indices, random_class2_indices])

X_new = X.iloc[final_indexes]
Y_new = Y.iloc[final_indexes]

print(X_new.shape)
print(Y_new.shape)

"""###**Using Accuracy Measure**"""

# Best Features using Decision Tree Classifier

best_feat_dtc = bi_direc_fsg(X.copy(),Y.copy(),DecisionTreeClassifier(),accuracy)

best_feat_svm = bi_direc_fsg(X_new.copy(),Y_new.copy(),SVC(),accuracy)

print("Using Accuracy Measure\n")
print("Best Features Selected by Decision Tree Classifier..\n", best_feat_dtc)
print("\nBest Features Selected by SVC..\n", best_feat_svm)

"""###**Using Information Measure**"""

#Information Gain
best_feat_ig = bi_direc_fsg(X.copy(),Y.copy(),DecisionTreeClassifier(), info_gain)

print("Using Information Measure\n")
print("Best Features Selected by Decision Tree Classifier and Information Gain..\n", best_feat_ig)

"""###**Using Distance Measure**

"""

#Eucledian Distance
best_feat_euc = bi_direc_fsg(X.copy(),Y.copy(),DecisionTreeClassifier(), euc_dist)

#City-block distance
best_feat_cb = bi_direc_fsg(X.copy(),Y.copy(),DecisionTreeClassifier(), CB_distance)

#Angular separation distance
best_feat_ang = bi_direc_fsg(X.copy(),Y.copy(),DecisionTreeClassifier(), ang_seperation)

print("Using Distance Measure")
print("\nBest Features Selected by Decision Tree Classifier and Angular Separation..\n", best_feat_euc)
print("\nBest Features Selected by Decision Tree Classifier and Euclidian Distance..\n", best_feat_cb)
print("\nBest Features Selected by Decision Tree Classifier and City-Block Distance..\n", best_feat_ang)

from sklearn.model_selection import cross_val_score

#Computing accuracy with Decision Tree as clssifier
acc_dtc = np.mean(cross_val_score(DecisionTreeClassifier(), X[best_feat_dtc], Y))
acc_svc = np.mean(cross_val_score(DecisionTreeClassifier(), X[best_feat_svm], Y))
im_ig = np.mean(cross_val_score(DecisionTreeClassifier(), X[best_feat_ig], Y))
dist_euc = np.mean(cross_val_score(DecisionTreeClassifier(), X[best_feat_euc], Y))
dist_cb = np.mean(cross_val_score(DecisionTreeClassifier(), X[best_feat_cb], Y))
dist_ang = np.mean(cross_val_score(DecisionTreeClassifier(), X[best_feat_ang], Y))

print(f"Accuracy with Accuracy measure: {acc_dtc}")
print(f"\nAccuracy with Information gain measure: {acc_svc}")
print(f"\nAccuracy with Euclidean distance measure: {im_ig}")
print(f"\nAccuracy with City-block distance measure: {dist_euc}")
print(f"\nAccuracy with Angular separation measure: {dist_cb}")
print(f"\nAccuracy with Angular separation measure: {dist_ang}")

import matplotlib.pyplot as plt
y = [acc_dtc, acc_svc, im_ig, dist_euc, dist_cb, dist_ang]
z = list(range(1, 7))
n = ["acc_dtc", "acc_svc", "info_gain", "euc_dist", "CB_dist", "Ang_Dist"]

fig, ax = plt.subplots()
fig.set_figheight(5)
fig.set_figwidth(15)

ax.scatter(z, y, s = 100)
ax.title.set_text("Accuracies of Features Selections")

for i, txt in enumerate(n):
    ax.annotate(txt, (z[i], y[i]))



"""#**Question2**

###Part 1
"""

#Making the dataset using numpy.multivariate_normal

#Given Covariace Matrix

cov = np.array([[0.6006771, 0.14889879, 0.244939], 
                [0.14889879, 0.58982531, 0.24154981], 
                [0.244939,0.24154981, 0.48778655 ]])

mean = np.zeros(cov.shape[0])

X = np.random.multivariate_normal(mean, cov, 1000)
print("Covariance Matrix is :\n", cov)

print("\nSome datapoints\n", X[0:5])

v_vector = np.array([[1/np.sqrt(6)], [1/np.sqrt(6)], [-2/np.sqrt(6)]])

temp_data = np.dot(X, v_vector)

Y = np.where(temp_data > 0, 0, 1)
print(Y.shape)

import plotly.graph_objects as go
import numpy as np

fig = go.Figure(data=[go.Scatter3d(x=X[:, 0], y=X[:, 1], z=X[:, 2],mode='markers',
                                  marker=dict(size=3, color=Y.reshape(-1, 1)))])
fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))
fig.show()

from sklearn.decomposition import PCA as PrincipalComponentAnalysis

PCA = PrincipalComponentAnalysis(n_components = 3)

PCA.fit(X)
X_trans_PCA = PCA.transform(X)

import plotly.graph_objects as go
import numpy as np

fig = go.Figure(data=[go.Scatter3d(x=X_trans_PCA[:, 0], y=X_trans_PCA[:, 1], z=X_trans_PCA[:, 2],mode='markers',
                                  marker=dict(size=3, color=Y.reshape(-1, 1)))])
fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))
fig.show()

#function to create substes

import itertools
def findsubsets(n, k):
    nums = list(range(n))
    return list(itertools.combinations(nums, k))

from sklearn.tree import DecisionTreeClassifier
from sklearn.inspection import DecisionBoundaryDisplay

subsets = findsubsets(X.shape[1], 2)

for subs in subsets:

    X_temp = X[:, subs]
    
    DTC = DecisionTreeClassifier(max_depth = 6)
    DTC.fit(X_temp, Y)
    disp = DecisionBoundaryDisplay.from_estimator(DTC, X_temp, response_method="predict",
                xlabel="Feature {}".format(subs[0]+1), ylabel="Feature {}".format(subs[1]+1),alpha=0.5)

    disp.ax_.scatter(X_temp[:, 0], X_temp[:, 1], c=Y, edgecolor="k", s=10, alpha = 0.8)

PCA2 = PrincipalComponentAnalysis(n_components = 2)

X_trans_PCA2 = PCA2.fit_transform(X)

print("Principal Components for n_components = 3 : \n", PCA.components_)
print("\nPrincipal Components for n_components = 2 : \n", PCA2.components_)

"""So, it selected Feature 1 and Feature 2"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold

kf = KFold(n_splits = 5)
# print(X_trans_PCA.shape)

for subs in subsets:
    temp_score = []
    for train_index, test_index in (kf.split(X_trans_PCA)):
        X_train = (X_trans_PCA[:, subs])[train_index]
        Y_train = Y[train_index]
        X_test = X_trans_PCA[:, subs][test_index]
        Y_test = Y[test_index]

        DTC = DecisionTreeClassifier()
        DTC.fit(X_train, Y_train)
        temp_score.append(DTC.score(X_test, Y_test))
    print("The Accuray Score for Feature-{} and Feature-{} is : {}".format(subs[0]+1, subs[1]+1, sum(temp_score)/len(temp_score)))

#finding correlation between each features

for i in range(3):
    print("\nCorrelation of Feature {} with Y :\n {}".format(i+1, np.corrcoef(X_trans_PCA[:,i], Y.reshape(1, -1))))

"""In this figure we can that Feature 3 has maxmimum covariance with Y.
So, it should have high preference to be selected.

By if we select, Feature 1 and Feature 2, then it will have relatively less relation with Y. So, they will not give better results.

Also, as seen in the scatter plots and decision boundary, we can see that feature 1 and feature 2, do not seperate the data, hence it is gives lower accuracies.

"""