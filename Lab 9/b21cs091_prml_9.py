# -*- coding: utf-8 -*-
"""B21CS091_PRML_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13bRKJPxtZ7NZWo1L62bhiu5zvEFPE6kA
"""

import torchvision
import numpy

from torchvision import datasets

train_data = datasets.MNIST(root='./data', train=True, download=True, transform=None)
test_data= datasets.MNIST(root='./data', train=False, download=True, transform=None)

print("Length of Training Data :", len(train_data))
print("Length of Testing Data :", len(test_data))

mean = (train_data.data.float().mean()/255).numpy()
std = (train_data.data.float().std()/255).numpy()

print("The mean of the data is :", mean)
print("The STD of the data is :", std)

from torchvision import transforms

train_trans = transforms.Compose([transforms.RandomRotation(5), transforms.RandomCrop(size = 28, padding = 2), 
                                  transforms.ToTensor(), transforms.Normalize(mean = mean, std = std)])
test_trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = [mean], std = [std])])

train_data = datasets.MNIST(root='./data', train=True, download=True, transform=train_trans)
test_data_fin= datasets.MNIST(root='./data', train=False, download=True, transform=test_trans)


print("Length of Training Data :", len(train_data))
print("Length of Testing Data :", len(test_data_fin))

import torch.utils.data as data
no_train = int(0.9*len(train_data))
no_val = int(0.1*len(train_data))

train_data_fin, val_data_fin = data.random_split(train_data, [no_train, no_val])

print("Length of Training Data :", len(train_data_fin))
print("Length of Validating Data :", len(val_data_fin))

import matplotlib.pyplot as plt
import numpy as np
import torchvision


class_images = [[] for i in range(10)]

for i, (image, label) in enumerate(train_data_fin):
    if len(class_images[label]) < 5:
        class_images[label].append(image)
    if all(len(images) == 5 for images in class_images):
        break

# Plot the images
fig, axs = plt.subplots(10, 5, figsize=(15, 15))
for i in range(10):
    for j in range(5):
        axs[i][j].imshow(np.squeeze(class_images[i][j]), cmap='gray')
        axs[i][j].axis('off')
plt.show()



import torch

train_loader = torch.utils.data.DataLoader(train_data_fin, batch_size=64, shuffle = True)
val_loader = torch.utils.data.DataLoader(val_data_fin, batch_size=64, shuffle = True)
test_loader = torch.utils.data.DataLoader(test_data_fin, batch_size=64, shuffle = True)

examples = enumerate(train_loader)
batch_idx, (example_data, example_targets) = next(examples)

import matplotlib.pyplot as plt

fig = plt.figure(figsize = (15, 10))

for i in range(32):
  plt.subplot(4, 8,i+1)
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("True Value: {}".format(example_targets[i]))
  plt.xticks([])
  plt.yticks([])

import torch
import torch.nn as nn

class MLP(nn.Module):

    def __init__(self, input_dim, output_dim):
        super(MLP, self).__init__()
        self.input_fc = nn.Linear(input_dim, 250)
        self.hidden_fc = nn.Linear(250, 100)
        self.output_fc = nn.Linear(100, output_dim)
        

    def forward(self, X):
        batch_size = X.shape[0]
        X = X.view(batch_size, -1)

        h1 = torch.relu(self.input_fc(X))
        h2 = torch.relu(self.hidden_fc(X))
        
        Y_pred = self.output_fc(h2)
        return Y_pred, h2

inp = 28*28
out = 10

model = MLP(inp, out)

def count_params(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print("Number of Trainanble Parameters :", count_params(model))

optimizer = torch.optim.Adam(model.parameters())

criteria = nn.CrossEntropyLoss()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
criteria = criteria.to(device)
print("The device used is:", device)

def cal_acc(y_pred, y):
    top_pred = y_pred.argmax(1, keepdim = True)
    correct = top_pred.eq(y.view_as(top_pred)).sum()
    acc = correct.float()/y.shape[0]
    return acc

def train(model, iter, optimizer, criteria, device):
    epoch_loss = 0
    epoch_acc = 0

    for (x, y) in iter:
        x = x.to(device)

        y = y.to(device)

        Y_pred, _ = model(x)
        loss = criteria(Y_pred, y)

        acc = cal_acc(Y_pred, y)

        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        eopch_acc = acc.item()

    return epoch_loss/len(iter), epoch_acc/len(iter)





"""#**ANN From Scratch** """

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics import accuracy_score

data = pd.read_csv('/content/abalone.data', header = None)

print(data.shape)

"""###**Visualization and PreProcessing**

"""

data.head()

data.describe()

sns.heatmap(data.isnull(),yticklabels=False)

sns.heatmap(data.corr())

import matplotlib.pyplot as plt

columns = list(data.columns)

f = plt.figure()
f.set_figwidth(15)
f.set_figheight(8)

for i in range(9):
    plt.subplot(2, 5, i+1)
    plt.title(columns[i])
    plt.hist(data[columns[i]])

plt.show()

Y = data.loc[:, 8]
X = data.drop(columns = [8]).copy()


X_temp = np.array(X[0])
X_temp[X_temp == 'M'] = 0.0
X_temp[X_temp == 'F'] = 1.0
X_temp[X_temp == 'I'] = 2.0

X[0] = X_temp

X.head()

Y.value_counts()

Y = np.array(Y)
X = np.array(X)

columns = [27, 24, 1, 26, 29, 2 , 25]

for i in columns:
    while(np.count_nonzero(Y == i) < 3):
        X = np.append(X, [X[np.where(Y == i)][0]], axis = 0)
        Y = np.append(Y, [i], axis = 0)

unique, counts = np.unique(Y, return_counts=True)

print(np.asarray((unique, counts)).T)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

from sklearn.model_selection import StratifiedShuffleSplit

sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=0)

train_index = []
test_index = []
for x, y in sss.split(X, Y):
    train_index = x
    test_index = y

X_train = X[train_index]
Y_train = Y[train_index]
X_test = sc.fit_transform(X[test_index])
Y_test = Y[test_index]

sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.125, random_state=0)

train_index = []
val_index = []

for x, y in sss.split(X_train, Y_train):
    train_index = x
    val_index = y

X_train = sc.fit_transform(X[train_index])
Y_train = Y[train_index]
X_val = sc.fit_transform(X[val_index])
Y_val = Y[val_index]

X_train = sc.fit_transform(X_train)
print(X_train.shape)
print(X_test.shape)
print(X_val.shape)
print(Y_train.shape)
print(Y_test.shape)
print(Y_val.shape)



"""###**Multi-Level Perceptron**"""

import random

class MultiLayerPerceptron: 
    def __init__(self, params=None):     

        self.inputLayer = params['InputLayer']
        self.hiddenLayer = params['HiddenLayer']
        self.OutputLayer = params['OutputLayer']
        self.learningRate = params['LearningRate']
        self.max_epochs = params['Epocas']
        self.BiasHiddenValue = params['BiasHiddenValue']
        self.BiasOutputValue = params['BiasOutputValue']
        self.activation = self.ativacao[params['ActivationFunction']]
        self.deriv = self.derivada[params['ActivationFunction']]
        
        self.WEIGHT_hidden = params['hidden_wight']
        self.WEIGHT_output = params['output_wight']

        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])
        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])
        self.classes_number = self.OutputLayer
        

    ativacao = { 'sigmoid': (lambda x: 1/(1.0 + np.exp(-x))), 'tanh': (lambda x: np.tanh(x)), 'Relu': (lambda x: x*(x > 0))}
    derivada = { 'sigmoid': (lambda x: x*(1-x)),'tanh': (lambda x: 1-x**2), 'Relu': (lambda x: 1 * (x>0)) }
 
    def Backpropagation_Algorithm(self, x):
        DELTA_output = []
        ERROR_output = self.output - self.OUTPUT_L2
        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))

        
        arrayStore = []

        for i in range(self.hiddenLayer):
            for j in range(self.OutputLayer):
                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))
                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])
      
        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)
 
        for i in range(self.inputLayer):
            for j in range(self.hiddenLayer):
                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))
                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])
                
    def make_error_plot(self,v_erro,v_epoca):
        plt.figure(figsize=(9,4))
        plt.plot(v_epoca, v_erro, "m-",color="b", marker=11)
        plt.xlabel("Number of Epochs")
        plt.ylabel("Squared error (MSE) ");
        plt.title("Error Minimization")
        plt.show()

    def predict(self, X):

        my_predictions = []

        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden
        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output
                                 
        for i in forward:
            my_predictions.append(max(enumerate(i), key=lambda x:x[1])[0])
            
        return my_predictions

    def fit(self, X, y):  
        count_epoch = 1
        total_error = 0
        n = len(X); 
        epoch_array = []
        error_array = []
        W0 = []
        W1 = []
        prev_error = 100
        while(count_epoch <= self.max_epochs):
            for idx,inputs in enumerate(X): 
                self.output = np.zeros(self.classes_number)
                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))
                self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))

                self.output[y[idx]-1] = 1
                
                square_error = 0
                for i in range(self.OutputLayer):
                    erro = (self.output[i] - self.OUTPUT_L2[i])**2
                    square_error = (square_error + (0.05 * erro))
                    total_error = total_error + square_error
         
                self.Backpropagation_Algorithm(inputs)
                
            total_error = (total_error / n)

            if prev_error <= total_error:
                break
            
            prev_error = total_error

            if(count_epoch % 5 == 0 or count_epoch == 1):
                print("Epoch ", count_epoch, "-> Total Error: ",total_error)
                error_array.append(total_error)
                epoch_array.append(count_epoch)
                
            W0.append(self.WEIGHT_hidden)
            W1.append(self.WEIGHT_output)
                
            count_epoch += 1
        self.make_error_plot(error_array,epoch_array)



"""**Using Tanh function**"""

params1 = {'InputLayer':8, 'HiddenLayer':5, 'OutputLayer':29,
              'Epocas':50, 'LearningRate':0.005,'BiasHiddenValue':-1, 
              'BiasOutputValue':-1, 'ActivationFunction':'tanh', 'hidden_wight': [[2  * random.random() - 1 for i in range(5)] for j in range(8)], 
              'output_wight':[[2  * random.random() - 1 for i in range(29)] for j in range(5)]}

MLP1 = MultiLayerPerceptron(params1)

MLP1.fit(X_train, Y_train)

Y_pred1 = MLP1.predict(X_test)

print("Accuracy with Tanh Activation Function is :", end = '')
print("%.3f" % (accuracy_score(Y_pred1, Y_test)*100), '%', sep = '')



"""**Using Relu Function**"""

params2 = {'InputLayer':8, 'HiddenLayer':5, 'OutputLayer':29,
              'Epocas':50, 'LearningRate':0.005,'BiasHiddenValue':-1, 
              'BiasOutputValue':-1, 'ActivationFunction':'Relu', 'hidden_wight': [[2  * random.random() - 1 for i in range(5)] for j in range(8)], 
              'output_wight':[[2  * random.random() - 1 for i in range(29)] for j in range(5)]}

MLP2 = MultiLayerPerceptron(params2)

MLP2.fit(X_train, Y_train)

Y_pred2 = MLP2.predict(X_test)

print("Accuracy with Relu Activation Function is :", end = '')
print("%.3f" % (accuracy_score(Y_pred2, Y_test)*100), '%', sep = '')



"""**Using Sigmoid Function**"""

params3 = {'InputLayer':8, 'HiddenLayer':5, 'OutputLayer':29,
              'Epocas':50, 'LearningRate':0.005,'BiasHiddenValue':-1, 
              'BiasOutputValue':-1, 'ActivationFunction':'sigmoid', 'hidden_wight': [[2  * random.random() - 1 for i in range(5)] for j in range(8)], 
              'output_wight':[[2  * random.random() - 1 for i in range(29)] for j in range(5)] }

MLP3 = MultiLayerPerceptron(params3)

MLP3.fit(X_train, Y_train)

Y_pred3 = MLP3.predict(X_test)

print("Accuracy with Sigmoid Activation Function is :", end = '')
print("%.3f" % (accuracy_score(Y_pred3, Y_test)*100), '%', sep = '')



"""**Using Different Zero Weight Initialization**"""

params4 = {'InputLayer':8, 'HiddenLayer':5, 'OutputLayer':29,
              'Epocas':50, 'LearningRate':0.005,'BiasHiddenValue':-1, 
              'BiasOutputValue':-1, 'ActivationFunction':'tanh', 'hidden_wight': np.zeros((8, 5)), 
              'output_wight':np.zeros((5, 29)) }

MLP4 = MultiLayerPerceptron(params4)

MLP4.fit(X_train, Y_train)

Y_pred4 = MLP4.predict(X_test)

print("Accuracy with Zero wieght Initialization and Tanh Function is :", end = '')
print("%.3f" % (accuracy_score(Y_pred4, Y_test)*100), '%', sep = '')





"""**Using Constant Weight Initialization**"""

params5 = {'InputLayer':8, 'HiddenLayer':5, 'OutputLayer':29,
              'Epocas':50, 'LearningRate':0.005,'BiasHiddenValue':-1, 
              'BiasOutputValue':-1, 'ActivationFunction':'tanh', 'hidden_wight': np.zeros((8, 5))+0.125, 
              'output_wight':np.zeros((5, 29)) + 0.20}

MLP5 = MultiLayerPerceptron(params5)

MLP5.fit(X_train, Y_train)

Y_pred5 = MLP5.predict(X_test)

print("Accuracy with Constant wieght Initialization and Tanh Function is :", end = '')
print("%.3f" % (accuracy_score(Y_pred5, Y_test)*100), '%', sep = '')



"""**Using Different Hidden Layers**"""

hidden_nodes = [10, 15, 20, 25]
accuracies = []

for h_n in hidden_nodes:
    print("Fitting Model for {} Hidden Nodes".format(h_n))
    params_new = {'InputLayer':8, 'HiddenLayer':h_n, 'OutputLayer':29,
              'Epocas':15, 'LearningRate':0.005,'BiasHiddenValue':-1, 
              'BiasOutputValue':-1, 'ActivationFunction':'tanh', 'hidden_wight':[[2  * random.random() - 1 for i in range(h_n)] for j in range(8)], 
              'output_wight':[[2  * random.random() - 1 for i in range(29)] for j in range(h_n)]}

    new_MLP = MultiLayerPerceptron(params_new)

    new_MLP.fit(X_train, Y_train)
    Y_pred_new = new_MLP.predict(X_test)

    accuracies.append(accuracy_score(Y_pred_new, Y_test))

for i in range(len(accuracies)):
    print("Accuracies with {} Hidden Nodes : {}".format(hidden_nodes[i], accuracies[i]))

import matplotlib.pyplot as plt

plt.plot(hidden_nodes, accuracies)
plt.xlabel('Hidden Nodes')
plt.ylabel('Accuracy')
plt.title("Hidden Nodes v/s Accuracy")

"""So, in general here, Accuracy decreases with increase in number if Hidden Nodes"""

